{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERT: Indexing & Search Notebook v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the relevant classes. As we'll see below, `Indexer` and `Searcher` are the key actors here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from colbert.infra import Run, RunConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow here assumes an IR dataset: a set of queries and a corresponding collection of passages.\n",
    "\n",
    "The classes `Queries` and `Collection` provide a convenient interface for working with such datasets.\n",
    "\n",
    "We'll load the answer posts of the English Language Learners (ELL) StackExchange community as our collection, and use relevant GooAQ questions as our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 21, 19:49:07] #> Loading the queries from /future/u/okhattab/data/tmp/stackexchange/ell/questions.tsv ...\n",
      "[Sep 21, 19:49:07] #> Got 441 queries. All QIDs are unique.\n",
      "\n",
      "[Sep 21, 19:49:07] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 441 queries and 214,300 passages'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = '/future/u/okhattab/data/tmp/stackexchange/'\n",
    "dataset = 'ell'\n",
    "\n",
    "queries = os.path.join(dataroot, dataset, 'questions.tsv')\n",
    "collection = os.path.join(dataroot, dataset, 'collection.answeronly.tsv')\n",
    "\n",
    "queries = Queries(path=queries)\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loaded 441 queries and 214,300 passages, numbered (in this dataset) from 0 onwathrough 440. Let's inspect one query and one passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are disease names proper nouns?\n",
      "\n",
      "No, “chance” and “get a chance” do not mean the same thing. “Get a chance” means “have an opportunity”, but the verb “chance” alone doesn't. The verb “chance” has several meanings, but none of them work here. In the sense of something happening by luck, it's normally used in a past tense (“I never chanced to meet him” = “I was not lucky enough to meet him”) or hypothetically (“If you chance to meet him, say hello from me” = “If you luck into meeting him, say hello from me”). It doesn't make sense here since by definition chance implies that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries[23])\n",
    "print()\n",
    "print(collection[79852])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "For efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
    "\n",
    "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index.\n",
    "\n",
    "(On future machines with four Titan V GPUs, indexing should take 6--7 minutes. The output is fairly ugly at the moment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"DocSettings\": {\n",
      "        \"dim\": 128,\n",
      "        \"doc_maxlen\": 220,\n",
      "        \"mask_punctuation\": true\n",
      "    },\n",
      "    \"IndexingSettings\": {\n",
      "        \"centroid_fraction_of_sample\": 0.03,\n",
      "        \"chunksize\": 2.0,\n",
      "        \"compression_level\": 1,\n",
      "        \"compression_thresholds\": \"\\/future\\/u\\/keshav2\\/compression_thresholds.csv\",\n",
      "        \"index_root\": null,\n",
      "        \"kmeans_niters\": 20,\n",
      "        \"kmeans_spherical\": true,\n",
      "        \"partitions\": null,\n",
      "        \"sample\": 0.05\n",
      "    },\n",
      "    \"QuerySettings\": {\n",
      "        \"query_maxlen\": 32\n",
      "    },\n",
      "    \"ResourceSettings\": {\n",
      "        \"checkpoint\": \"\\/dfs\\/scratch0\\/okhattab\\/OpenQA\\/colbert-400000.dnn\",\n",
      "        \"collection\": \"\\/future\\/u\\/okhattab\\/data\\/tmp\\/stackexchange\\/ell\\/collection.answeronly.tsv\",\n",
      "        \"index_name\": \"ell.index\",\n",
      "        \"queries\": null,\n",
      "        \"triples\": null\n",
      "    },\n",
      "    \"RunSettings\": {\n",
      "        \"amp\": true,\n",
      "        \"experiment\": \"notebook\",\n",
      "        \"gpus\": [\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3\n",
      "        ],\n",
      "        \"name\": \"2021-09-21\\/19.49.08\",\n",
      "        \"nranks\": 4,\n",
      "        \"overwrite\": false,\n",
      "        \"rank\": -1,\n",
      "        \"root\": \"\\/future\\/u\\/okhattab\\/repos\\/ColBERT-private-releases\\/experiments\"\n",
      "    },\n",
      "    \"SearchSettings\": {\n",
      "        \"faiss_depth\": 16384,\n",
      "        \"nprobe\": 2\n",
      "    },\n",
      "    \"TrainingSettings\": {\n",
      "        \"accumsteps\": 1,\n",
      "        \"bsize\": 64,\n",
      "        \"lr\": 3e-6,\n",
      "        \"maxsteps\": 500000,\n",
      "        \"resume\": false,\n",
      "        \"save_every\": null,\n",
      "        \"similarity\": \"cosine\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "[Sep 21, 19:49:08] #> Note: Output directory /future/u/okhattab/repos/ColBERT-private-releases/experiments/notebook/indexes/ell.index already exists\n",
      "\n",
      "\n",
      "#> Preparing...\n",
      "#> Preparing...\n",
      "#> Preparing...\n",
      "#> Preparing...\n",
      "[Sep 21, 19:49:09] [Pre-Emptying] GPU memory check: r=0, a=0, f=0\n",
      "[Sep 21, 19:49:09] [Post-Emptying] GPU memory check: r=0, a=0, f=0\n",
      "#> Starting...\n",
      "#> Starting...\n",
      "#> Starting...\n",
      "#> Starting...\n",
      "nranks = 4 \t num_gpus = 4 \t device=1\n",
      "[Sep 21, 19:50:07] [1] \t\t #> Encoding 18994 passages..\n",
      "nranks = 4 \t num_gpus = 4 \t device=3\n",
      "[Sep 21, 19:50:07] [3] \t\t #> Encoding 18983 passages..\n",
      "nranks = 4 \t num_gpus = 4 \t device=0\n",
      "[Sep 21, 19:50:07] [0] \t\t # of sampled PIDs = 81138 \t sampled_pids[:3] = [109214, 192069, 2665]\n",
      "nranks = 4 \t num_gpus = 4 \t device=2\n",
      "[Sep 21, 19:50:07] [2] \t\t #> Encoding 18845 passages..\n",
      "[Sep 21, 19:50:07] [0] \t\t #> Encoding 24316 passages..\n",
      "[Sep 21, 19:50:57] [0] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 24,316\n",
      "[Sep 21, 19:50:57] [2] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 18,845\n",
      "[Sep 21, 19:50:57] [3] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 18,983\n",
      "[Sep 21, 19:50:57] [1] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 18,994\n",
      "[Sep 21, 19:51:01] [0] \t\t Creaing 65,536 partitions. *Estimated* 16,925,162.93258667 embeddings.\n",
      "[Sep 21, 19:51:01] [0] \t\t #> Saving the indexing plan to /future/u/okhattab/repos/ColBERT-private-releases/experiments/notebook/indexes/ell.index/plan.json ..\n",
      "Clustering 6168239 points in 128D to 65536 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.65 s\n",
      "  Iteration 19 (138.63 s, search 133.16 s): objective=5.0599e+06 imbalance=1.333 nsplit=0        \n",
      "[0.041, 0.044, 0.043, 0.044, 0.046, 0.046, 0.044, 0.043, 0.043, 0.045, 0.045, 0.043, 0.041, 0.045, 0.043, 0.041, 0.044, 0.044, 0.044, 0.046, 0.043, 0.041, 0.047, 0.041, 0.046, 0.049, 0.044, 0.043, 0.043, 0.042, 0.043, 0.044, 0.045, 0.044, 0.044, 0.043, 0.039, 0.049, 0.042, 0.041, 0.043, 0.039, 0.043, 0.046, 0.043, 0.045, 0.045, 0.042, 0.043, 0.041, 0.044, 0.042, 0.042, 0.043, 0.042, 0.043, 0.04, 0.04, 0.041, 0.043, 0.042, 0.047, 0.044, 0.043, 0.05, 0.05, 0.046, 0.044, 0.043, 0.046, 0.04, 0.041, 0.043, 0.045, 0.045, 0.045, 0.044, 0.04, 0.044, 0.043, 0.044, 0.043, 0.041, 0.042, 0.043, 0.04, 0.041, 0.042, 0.044, 0.046, 0.041, 0.039, 0.046, 0.043, 0.043, 0.039, 0.041, 0.048, 0.042, 0.048, 0.042, 0.047, 0.044, 0.044, 0.042, 0.048, 0.042, 0.039, 0.04, 0.042, 0.044, 0.043, 0.043, 0.046, 0.044, 0.042, 0.041, 0.045, 0.042, 0.039, 0.043, 0.041, 0.039, 0.041, 0.045, 0.04, 0.042, 0.048]\n",
      "[Sep 21, 19:53:35] avg_residual = 0.043212890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 21, 19:53:36] [0] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:53:36] [2] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:53:36] [1] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:53:36] [3] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:54:24] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 2,050,051 embeddings. From #0 onward.\n",
      "[Sep 21, 19:54:32] [1] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:54:32] [3] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:54:33] [2] \t\t #> Encoding 25000 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:57, 57.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 21, 19:54:33] [0] \t\t #> Encoding 25000 passages..\n",
      "[Sep 21, 19:55:21] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 1,922,803 embeddings. From #100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:52, 56.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 21, 19:55:29] [0] \t\t #> Encoding 14300 passages..\n",
      "[Sep 21, 19:55:56] [0] \t\t #> Saving chunk 8: \t 14,300 passages and 1,225,125 embeddings. From #200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:25, 48.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sep 21, 19:56:10] [0] \t\t #> Saving the indexing metadata to /future/u/okhattab/repos/ColBERT-private-releases/experiments/notebook/indexes/ell.index/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n",
      "#> Joined...\n",
      "#> Joined...\n",
      "#> Joined...\n",
      "[Sep 21, 19:56:13] #> Loading collection...\n",
      "0M \n",
      "#> WARNING: StridedTensor has to add padding, internally, to a large tensor.\n",
      "#> WARNING: Consider doing this padding in advance to save memory!\n",
      "[Sep 21, 19:56:27] #> Building the emb2pid mapping..\n",
      "[Sep 21, 19:56:28] len(self.emb2pid) = 16976942\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=4, experiment='notebook')):  # nranks specifies the number of GPUs to use.\n",
    "    index_name = f'{dataset}.index'\n",
    "\n",
    "    indexer = Indexer(checkpoint='/dfs/scratch0/okhattab/OpenQA/colbert-400000.dnn')  # MS MARCO ColBERT checkpoint\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)\n",
    "\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
    "\n",
    "We can use the `queries` set we loaded earlier — or you can supply your own questions, say, \"how large is the vocabulary of the average person?\"\n",
    "\n",
    "Feel free to get creative! But keep in mind this set of ~200k ELL passages can only answer a small, focused set of questions on English Language Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> are morphology and structure the same?\n",
      "\t [1] \t\t 21.3 \t\t This is tricky one, I'll try and explain my perspective as a native British English speaker. I have a biology background as opposed to physics, so that might come into play as well here. Some defintions: Structure: Arrangement of and relations between the parts or elements of something complex or a piece of construction. Whereas morphology: Morphology: A particular form, shape, or structure or the study of something's form of shape. As you can see, they both have one usage where they are near enough synonymous, and another where they are not. In most situations they can be\n",
      "\t [2] \t\t 19.6 \t\t Since each program has one structure, the correct sentence is Program A and program B have the same structure: they both have a sequential structure. There is a single structure, which is shared by the two programs. The “same structure” in the first sentence is the sequential structure, there is only one. Other ways to formulate this idea include The structure of program A is the same as the structure of program B. Both program A and program B have a sequential structure. The structures of program A and program B are the same. In this\n",
      "\t [3] \t\t 19.4 \t\t The structure is of the form of X. The structure is in the form of X. What is the difference in meaning and which one is grammatically correct? I think 1 means the structure is united with the form of X and 2 means the structure is shaped in the form of X So, as for 1, I don't think the entire shape of the structure must look in the form of X (maybe ,part of it is united with the form of X?), but as for 2, I do think the entire shape of the\n"
     ]
    }
   ],
   "source": [
    "query = queries[30]   # or supply your own query\n",
    "\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=3)\n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Search\n",
    "\n",
    "In many applications, you have a large batch of queries and you need to maximize the overall throughput. For that, you can use the `searcher.search_all(queries, k)` method, which returns a `Ranking` object that organizes the results across all queries.\n",
    "\n",
    "(Batching provides many opportunities for higher-throughput search, though we have not implemented most of those optimizations for compressed indexes yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rankings = searcher.search_all(queries, k=5).todict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(54519, 1, 21.296875),\n",
       " (9029, 2, 19.625),\n",
       " (177893, 3, 19.4375),\n",
       " (156910, 4, 19.40625),\n",
       " (10821, 5, 19.34375)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings[30]  # For query 30, a list of (passage_id, rank, score) for the top-k passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
